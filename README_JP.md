# BERTã¨GPTã‚’0ï¸âƒ£ã‹ã‚‰æ§‹ç¯‰ã™ã‚‹
[ğŸ‡¬ğŸ‡§English Version](./README.md)

![](https://miro.medium.com/v2/resize:fit:1400/1*TzGwyi1TrqcIPV4WMU3sVg.png)

# ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼

## [BERT](./Encoder/BERT/)
1. [æ¦‚è¦](./Encoder/BERT/overview.ipynb)
2. [äº‹å‰å­¦ç¿’](./Encoder/BERT/pre-training.ipynb)
3. [ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°](./Encoder/BERT/fine-tuning.ipynb)
4. [å¯è¦–åŒ–-BertViz](https://github.com/jessevig/bertviz)

## ROBERTA
BERT vs RoBERTa
1. RoBERTaã¯ã‚ˆã‚Šå¤šãã®äº‹å‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
2. RoBERTaã¯MLMã‚¿ã‚¹ã‚¯ã®ã¿ã§å­¦ç¿’ã•ã‚Œã€å‹•çš„ãƒã‚¹ã‚­ãƒ³ã‚°ã‚’ä½¿ç”¨

# ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼

## GPT(`TODO`)
1. [æ¦‚è¦](./Decoder/GPT/overview.ipynb)
2. [äº‹å‰å­¦ç¿’](./Decoder/GPT/pre-training.ipynb)
3. [ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°](./Decoder/GPT/fine-tuning.ipynb)

## [Llama](./Decoder/Llama/) (`TODO`)
Llama vs GPT
    1. RoPE
    2. RMSNorm
    3. SwiGLU
    4. Flash Attention
    5. äº‹å‰æ­£è¦åŒ–

# ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ
```
git clone \\
conda create -n "bert_gpt_test" python = 3.12
conda activate bert_gpt_test
pip install -r requirements.txt
```

### ä»Šå¾Œã®äºˆå®š
1. Roberta vs BERT
2. ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼-ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼: T5, BART
3. GPT, Llama