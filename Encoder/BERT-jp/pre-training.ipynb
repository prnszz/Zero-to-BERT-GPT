{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe697a9b-6a11-489b-9701-15d42d3feb15_2006x1206.png)\n",
    "\n",
    "## マスク言語モデリング（MLM）：\n",
    "\n",
    "入力文の15%のトークンをランダムに[MASK]に置き換える。　　　\n",
    "- モデルはこれらのマスクされた単語を予測する。　　　\n",
    "- 例えば、「[MASK]が[MASK]を飛び越える」という文から「犬」「柵」を予測します。   \n",
    "\n",
    "-> これにより、BERTは文脈における単語の意味を理解できるようになります\n",
    "\n",
    "\n",
    "## 次文予測（NSP）：\n",
    "\n",
    "2つの文が与えられた時、文Bが文Aの実際の次の文であるかを予測する。\n",
    "例：\n",
    "- 文A：「男性は店に行きました」\n",
    "- 文B：「彼は牛乳を買いました」（関連あり、IsNextと予測）\n",
    "- 文B：「ペンギンは南極に住んでいます」（関連なし、NotNextと予測）\n",
    "\n",
    "-> これにより、BERTは文と文の関係性を理解できるようになります"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメータ設定\n",
    "maxlen = 30  \n",
    "batch_size = 6  \n",
    "max_pred = 5  \n",
    "n_layers = 6  \n",
    "n_heads = 12  \n",
    "d_model = 768  \n",
    "d_ff = 768*4  \n",
    "d_k = d_v = 64  \n",
    "n_segments = 2  \n",
    "num_epochs = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  パラメータ設定\n",
    "\n",
    "1. データ処理関連:   \n",
    "-  maxlen: 各文の最大トークン数, これより長い文は切り捨て, 短い文はパディング  \n",
    "- batch_size: 1回の学習更新で処理するサンプル数\n",
    "- max_pred: マスク予測の最大数, MLMタスクで予測する単語の最大数, 入力長の15%程度になるよう調整\n",
    "\n",
    "\n",
    "2. モデルアーキテクチャ\n",
    "\n",
    "- n_layers: モデルの深さ, Transformerレイヤーの数\n",
    "- n_heads: マルチヘッドアテンションのヘッド数, 並列処理の度合い\n",
    "- d_model: 表現力に関わる次元数, フィードフォワードネットワークの中間層の次元数, 通常、d_modelの4倍に設定\n",
    "- d_ff, d_k, d_v: キーと値のベクトル次元数, d_modelをヘッド数で割った値に設定されることが多い\n",
    "- n_segments: タスク固有の設定, NSPタスクで使用するセグメント種類数(文1と文2の区別)\n",
    "\n",
    "\n",
    "3. 学習設定\n",
    "\n",
    "- num_epochs: 学習の反復回数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/nlp/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'unk_token': '[UNK]',\n",
       " 'sep_token': '[SEP]',\n",
       " 'pad_token': '[PAD]',\n",
       " 'cls_token': '[CLS]',\n",
       " 'mask_token': '[MASK]'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# トークナイザの特殊トークンを確認\n",
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "special_tokens = tokenizer.special_tokens_map\n",
    "\n",
    "special_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特殊トークン\n",
    "\n",
    "1. [UNK] (Unknown Token)\n",
    "未知の単語を表すトークンだ。BERTの語彙（ボキャブラリ）に存在しない単語が入力された場合、自動的にこのトークンに置き換えられる。    \n",
    "\n",
    "2. [SEP] (Separator Token)\n",
    "テキストのセクションを分けるためのトークンだ。   \n",
    "例：2つの文を区別する際に使います。    \n",
    "文1: これは文だ。[SEP]    \n",
    "文2: こちらも別の文だ。    \n",
    "\n",
    "3. [PAD] (Padding Token)\n",
    "入力を一定の長さに揃えるためのトークンだ。短い文を埋める際に使用され、モデルには無視される。\n",
    "\n",
    "4. [CLS] (Classification Token)\n",
    "文全体の情報を要約するためのトークンだ。     \n",
    "例：文分類タスクでは、このトークンの出力を使ってクラスを予測する。   \n",
    "\n",
    "5. [MASK] (Mask Token)\n",
    "マスクされたトークンを表す。     \n",
    "主にBERTの事前学習（マスク付き言語モデル）で使用され、文中の一部を隠して予測するタスクで用いられる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: '[PAD]',\n",
       "  1: '[CLS]',\n",
       "  2: '[SEP]',\n",
       "  3: '[MASK]',\n",
       "  4: 'competition',\n",
       "  5: 'nice',\n",
       "  6: 'oh',\n",
       "  7: 'to',\n",
       "  8: 'you',\n",
       "  9: 'how',\n",
       "  10: 'today',\n",
       "  11: 'thank',\n",
       "  12: 'am',\n",
       "  13: 'baseball',\n",
       "  14: 'name',\n",
       "  15: 'i',\n",
       "  16: 'won',\n",
       "  17: 'congratulations',\n",
       "  18: 'meet',\n",
       "  19: 'great',\n",
       "  20: 'juliet',\n",
       "  21: 'the',\n",
       "  22: 'hello',\n",
       "  23: 'are',\n",
       "  24: 'romeo',\n",
       "  25: 'my',\n",
       "  26: 'team',\n",
       "  27: 'too',\n",
       "  28: 'is'},\n",
       " 29)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "text = (\n",
    "    'Hello, how are you? I am Romeo.\\n' \n",
    "    'Hello, Romeo My name is Juliet. Nice to meet you.\\n'\n",
    "    'Nice to meet you too. How are you today?\\n'\n",
    "    'Great. My baseball team won the competition.\\n'\n",
    "    'Oh Congratulations, Juliet\\n'\n",
    "    'Thank you Romeo'\n",
    ")\n",
    "\n",
    "# 文章を前処理：\n",
    "# - 句読点などを削除\n",
    "# - 小文字に変換\n",
    "# - 改行で分割\n",
    "sentences = re.sub(\"[.,!?-]\", '', text.lower()).split('\\n')  # filter '.', ',', '?', '!'\n",
    "\n",
    "word_list = list(set(\" \".join(sentences).split()))\n",
    "# 特殊トークンを含む単語辞書の初期化\n",
    "word_dict = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
    "for i, w in enumerate(word_list):\n",
    "   word_dict[w] = i + 4\n",
    "   number_dict = {i: w for i, w in enumerate(word_dict)}\n",
    "vocab_size = len(word_dict)\n",
    "number_dict, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello how are you i am romeo\n",
      "hello romeo my name is juliet nice to meet you\n",
      "nice to meet you too how are you today\n",
      "great my baseball team won the competition\n",
      "oh congratulations juliet\n",
      "thank you romeo\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[22, 9, 23, 8, 15, 12, 24],\n",
       " [22, 24, 25, 14, 28, 20, 5, 7, 18, 8],\n",
       " [5, 7, 18, 8, 27, 9, 23, 8, 10],\n",
       " [19, 25, 13, 26, 16, 21, 4],\n",
       " [6, 17, 20],\n",
       " [11, 8, 24]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list = []\n",
    "for sentence in sentences:\n",
    "   arr = [word_dict[s] for s in sentence.split()]\n",
    "   token_list.append(arr)\n",
    "   print(sentence)\n",
    "   # print(arr)\n",
    "\n",
    "token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange, shuffle, randint, random\n",
    "\n",
    "\n",
    "def make_batch():\n",
    "   batch = []\n",
    "   positive = negative = 0 # 連続する文の場合は正例、連続しない文の場合は負例、数をbatch_sizeの半分ずつ生成\n",
    "\n",
    "   # バッチサイズの半分ずつになるまでループ\n",
    "   while positive != batch_size/2 or negative != batch_size/2:\n",
    "       # ランダムに2つの文を選択\n",
    "       tokens_a_index, tokens_b_index= randrange(len(sentences)), randrange(len(sentences))\n",
    "\n",
    "       # 選択された文のトークン列を取得\n",
    "       tokens_a, tokens_b= token_list[tokens_a_index], token_list[tokens_b_index]\n",
    "\n",
    "       # 入力形式の作成：[CLS] + 1番目の文 + [SEP] + 2番目の文 + [SEP]\n",
    "       input_ids = [word_dict['[CLS]']] + tokens_a + [word_dict['[SEP]']] + tokens_b + [word_dict['[SEP]']]\n",
    "       # セグメントID: 1番目の文には0、2番目の文には1を割り当て\n",
    "       segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        # === マスク言語モデル（MLM）の処理 ===\n",
    "        # マスクする単語数を決定（文長の15%、ただし最大値と最小値で制限）\n",
    "       n_pred =  min(max_pred, max(1, int(round(len(input_ids) * 0.15)))) \n",
    "       \n",
    "        # マスク候補位置の選定（[CLS]と[SEP]以外の全ての位置）\n",
    "       cand_maked_pos = [i for i, token in enumerate(input_ids)\n",
    "                         if token != word_dict['[CLS]'] and token != word_dict['[SEP]']]\n",
    "       \n",
    "       shuffle(cand_maked_pos)\n",
    "       masked_tokens, masked_pos = [], []\n",
    "\n",
    "       # 選択された位置に対してマスク処理を実行\n",
    "       for pos in cand_maked_pos[:n_pred]:\n",
    "           masked_pos.append(pos)\n",
    "           masked_tokens.append(input_ids[pos])\n",
    "           if random() < 0.8:  # 80%の確率で[MASK]トークンに置き換え\n",
    "               input_ids[pos] = word_dict['[MASK]'] # make mask\n",
    "           elif random() < 0.5:  # 10%の確率でランダムな単語に置き換え\n",
    "               index = randint(0, vocab_size - 1) # random index in vocabulary\n",
    "               input_ids[pos] = word_dict[number_dict[index]] # replace\n",
    "\n",
    "\n",
    "       # === パディング処理 ===\n",
    "       # 入力系列を最大長(maxlen)まで0でパディング\n",
    "       n_pad = maxlen - len(input_ids)\n",
    "       input_ids.extend([0] * n_pad)\n",
    "       segment_ids.extend([0] * n_pad)\n",
    "\n",
    "       # マスクされたトークンも最大予測数(max_pred)まで0でパディング\n",
    "       if max_pred > n_pred:\n",
    "           n_pad = max_pred - n_pred\n",
    "           masked_tokens.extend([0] * n_pad)\n",
    "           masked_pos.extend([0] * n_pad)\n",
    "\n",
    "\n",
    "        # === Next Sentence Prediction (NSP) のラベル付け ===\n",
    "        # 連続する文の場合は正例として追加\n",
    "       if tokens_a_index + 1 == tokens_b_index and positive < batch_size/2:\n",
    "           batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True]) # IsNext\n",
    "           positive += 1\n",
    "\n",
    "        # 連続しない文の場合は負例として追加\n",
    "       elif tokens_a_index + 1 != tokens_b_index and negative < batch_size/2:\n",
    "           batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False]) # NotNext\n",
    "           negative += 1\n",
    "   return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs, batch[0][0]: [1, 5, 7, 18, 8, 27, 9, 23, 8, 10, 2, 6, 17, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Segment IDs, batch[0][1]: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Masked Tokens, batch[0][2]: [20, 10, 0, 0, 0]\n",
      "Masked Positions, batch[0][3]: [13, 9, 0, 0, 0]\n",
      "Is Next, batch[0][4]: False\n",
      "\n",
      "Input IDs, batch[1][0]: [1, 19, 25, 13, 26, 16, 21, 4, 2, 3, 17, 20, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Segment IDs, batch[1][1]: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Masked Tokens, batch[1][2]: [19, 6, 0, 0, 0]\n",
      "Masked Positions, batch[1][3]: [1, 9, 0, 0, 0]\n",
      "Is Next, batch[1][4]: True\n",
      "\n",
      "Input IDs, batch[2][0]: [1, 6, 17, 20, 2, 11, 8, 24, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Segment IDs, batch[2][1]: [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Masked Tokens, batch[2][2]: [8, 0, 0, 0, 0]\n",
      "Masked Positions, batch[2][3]: [6, 0, 0, 0, 0]\n",
      "Is Next, batch[2][4]: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a batch\n",
    "batch = make_batch()\n",
    "\n",
    "# バッチの内容を確認\n",
    "for b in batch[:3]:\n",
    "    input_ids, segment_ids, masked_tokens, masked_pos, isNext = b\n",
    "    index = batch.index(b)\n",
    "    print(f\"Input IDs, batch[{index}][0]: {input_ids}\")\n",
    "    print(f\"Segment IDs, batch[{index}][1]: {segment_ids}\")\n",
    "    print(f\"Masked Tokens, batch[{index}][2]: {masked_tokens}\")\n",
    "    print(f\"Masked Positions, batch[{index}][3]: {masked_pos}\")\n",
    "    print(f\"Is Next, batch[{index}][4]: {isNext}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# マスクされたトークンのattention maskを作成\n",
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)   # batch_size x 1 x len_k(=len_q), one is masking\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queryの (seq_q):\n",
      "tensor([[1, 2, 3, 0],\n",
      "        [4, 5, 0, 0]])\n",
      "\n",
      "keyの (seq_k):\n",
      "tensor([[1, 2, 3, 0],\n",
      "        [4, 5, 0, 0]])\n",
      "\n",
      "結果 (pad_mask):\n",
      "tensor([[[False, False, False,  True],\n",
      "         [False, False, False,  True],\n",
      "         [False, False, False,  True],\n",
      "         [False, False, False,  True]],\n",
      "\n",
      "        [[False, False,  True,  True],\n",
      "         [False, False,  True,  True],\n",
      "         [False, False,  True,  True],\n",
      "         [False, False,  True,  True]]])\n"
     ]
    }
   ],
   "source": [
    "# get_attn_pad_mask関数の動作確認\n",
    "# get_attn_pad_mask(torch.tensor([batch[0][0]]), torch.tensor([batch[0][0]]))\n",
    "\n",
    "seq_q = torch.tensor([[1, 2, 3, 0], [4, 5, 0, 0]])  # batch_size x len_q\n",
    "seq_k = torch.tensor([[1, 2, 3, 0], [4, 5, 0, 0]])  # batch_size x len_k\n",
    "pad_mask = get_attn_pad_mask(seq_q, seq_k)\n",
    "print(\"queryの (seq_q):\")\n",
    "print(seq_q)\n",
    "print(\"\\nkeyの (seq_k):\")\n",
    "print(seq_k)\n",
    "print(\"\\n結果 (pad_mask):\")\n",
    "print(pad_mask)\n",
    "# 結果のTrueのところは、パディングされた位置を示す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "\n",
    "# リスト　-> テンソル\n",
    "batch = make_batch()\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = zip(*batch)\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = (\n",
    "    torch.LongTensor(input_ids),\n",
    "    torch.LongTensor(segment_ids),\n",
    "    torch.LongTensor(masked_tokens),\n",
    "    torch.LongTensor(masked_pos),\n",
    "    torch.LongTensor(isNext)\n",
    ")\n",
    "\n",
    "# pytorchを使って、batch処理のため、データセットを作成\n",
    "class MyDataSet(Data.Dataset):\n",
    "    def __init__(self, input_ids, segment_ids, masked_tokens, masked_pos, isNext):\n",
    "        self.input_ids = input_ids\n",
    "        self.segment_ids = segment_ids\n",
    "        self.masked_tokens = masked_tokens\n",
    "        self.masked_pos = masked_pos\n",
    "        self.isNext = isNext\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.segment_ids[idx], self.masked_tokens[idx], self.masked_pos[idx], self.isNext[idx]\n",
    "    \n",
    "loader = Data.DataLoader(MyDataSet(input_ids, segment_ids, masked_tokens, masked_pos, isNext), batch_size, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture\n",
    "![](https://humboldt-wi.github.io/blog/img/seminar/bert/bert_architecture.png)\n",
    "もっと詳しいコードは[こちら](https://github.com/huggingface/transformers/blob/31d452c68b34c2567b62924ee0df40a83cbc52d5/src/transformers/models/bert/modeling_bert.py#L187)に参照してください。\n",
    "BERTの全部のclassはこのサイトにあると考えられる: \n",
    "- `BertEmbeddings`: 単語、位置、およびトークンタイプの埋め込みを構築します。\n",
    "- `BertSelfAttention`: 自己注意メカニズムを実装します。\n",
    "- `BertSelfOutput`: デンスレイヤーを適用し、入力テンソルを加算します。\n",
    "- `BertAttention`: 自己注意と出力レイヤーを組み合わせます。\n",
    "- `BertIntermediate`: 活性化関数を持つデンスレイヤーを適用します。\n",
    "- `BertOutput`: もう一つのデンスレイヤーを適用し、入力テンソルを加算します。\n",
    "- `BertLayer`: 注意層と中間出力層を組み合わせます。\n",
    "- `BertEncoder`: 複数のBertLayerインスタンスをスタックします。\n",
    "- `BertPooler`: デンスレイヤーを適用し、Tanh活性化を行います。\n",
    "- `BertPredictionHeadTransform`: デンスレイヤーと活性化関数を適用します。\n",
    "- `BertLMPredictionHead`: マスクされた言語モデリングの予測を生成します。\n",
    "- `BertOnlyMLMHead`: マスクされた言語モデリングヘッドのみを含みます。\n",
    "- `BertOnlyNSPHead`: 次の文予測ヘッドのみを含みます。\n",
    "- `BertPreTrainingHeads`: MLMとNSPヘッドを組み合わせます。\n",
    "- `BertPreTrainedModel`: すべてのBERTモデルの基本クラスです。\n",
    "- `BertForPreTrainingOutput`: 事前トレーニングの出力タイプです。\n",
    "- `BertModel`: 特定のヘッドなしのメインBERTモデルです。\n",
    "- `BertForPreTraining`: MLMとNSPヘッドを持つBERTモデルです。\n",
    "- `BertLMHeadModel`: 言語モデリングヘッドを持つBERTモデルです。\n",
    "- `BertForMaskedLM`: マスクされた言語モデリングヘッドを持つBERTモデルです。\n",
    "- `BertForNextSentencePrediction`: 次の文予測のためのBERTモデルです。\n",
    "- `BertForSequenceClassification`: シーケンス分類のためのBERTモデルです。\n",
    "- `BertForMultipleChoice`: 複数選択タスクのためのBERTモデルです。\n",
    "- `BertForTokenClassification`: トークン分類タスクのためのBERTモデルです。\n",
    "- `BertForQuestionAnswering`: 質問応答タスクのためのBERTモデルです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ BERT = Embedding + N \\times EncoderLayer $$  \n",
    "\n",
    "(Embedding -> (MultiHeadAttention + PoswiseFeedForwardNet) == EncoderLayer $\\times$ N) == BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.tok_embed = nn.Embedding(vocab_size, d_model)\n",
    "        # transformers libraryのBERTはcos位置三角関数を使ってない\n",
    "        # https://github.com/huggingface/transformers/blob/31d452c68b34c2567b62924ee0df40a83cbc52d5/src/transformers/models/bert/modeling_bert.py\n",
    "        # hugging face コミュニティの説明\n",
    "        # https://discuss.huggingface.co/t/why-positional-embeddings-are-implemented-as-just-simple-embeddings/585/5\n",
    "        self.pos_embed = nn.Embedding(maxlen, d_model)\n",
    "        self.seg_embed = nn.Embedding(n_segments, d_model) # トークンのセグメント情報をエンコード(文0 or 文1)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, input_ids, segment_ids):\n",
    "        seq_len = input_ids.size(1)\n",
    "        # posは0から始まる, 0, 1, 2, 3, ..., seq_len-1\n",
    "        pos = torch.arange(seq_len, dtype=torch.long)\n",
    "        pos = pos.unsqueeze(0).expand_as(input_ids)\n",
    "        embedding = self.tok_embed(input_ids) + self.pos_embed(pos) + self.seg_embed(segment_ids)\n",
    "        \n",
    "        return self.norm(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
    "\n",
    "        \n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
    "        residual, batch_size = Q, Q.size(0)\n",
    "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)\n",
    "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)\n",
    "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)\n",
    "\n",
    "        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        context, attn = nn.functional.scaled_dot_product_attention(q_s, k_s, v_s, attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v)\n",
    "        output = nn.Linear(n_heads * d_v, d_model)(context)\n",
    "        \n",
    "        return nn.LayerNorm(d_model)(output + residual), attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # geluは活性化関数、Reluの改良版、\n",
    "        return self.fc2(nn.GELU()(self.fc1(inputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
    "        enc_outputs = self.layer_norm1(enc_inputs + enc_outputs) # Add & Norm\n",
    "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size x len_q x d_model]\n",
    "        enc_outputs = self.layer_norm2(enc_outputs + ffn_outputs) # Add & Norm\n",
    "        \n",
    "        return enc_outputs, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERT, self).__init__()\n",
    "        self.embedding = Embedding()\n",
    "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Tanh(), # BERTのpooling層はTanh\n",
    "        )\n",
    "        self.classifier = nn.Linear(d_model, 2)\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.activ2 = nn.GELU()\n",
    "        embed_weight = self.embedding.tok_embed.weight\n",
    "        self.fc2 = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.fc2.weight = embed_weight\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, masked_pos):\n",
    "        output = self.embedding(input_ids, segment_ids)\n",
    "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids)\n",
    "        for layer in self.layers:\n",
    "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
    "        h_pooled = self.fc(output[:, 0])\n",
    "        logits_clsf = self.classifier(h_pooled)\n",
    "\n",
    "        masked_pos = masked_pos[:, :, None].expand(-1, -1, d_model)  # [batch_size, max_pred, d_model]\n",
    "        h_masked = torch.gather(output, 1, masked_pos)\n",
    "        h_masked = self.activ2(self.linear(h_masked))\n",
    "        logits_lm = self.fc2(h_masked)\n",
    "        \n",
    "        return logits_lm, logits_clsf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../../image/BERT_arch.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERT()\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for input_ids, segment_ids, masked_tokens, masked_pos, isNext in loader:\n",
    "        logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
    "        loss_lm = criteria(logits_lm.transpose(1, 2), masked_tokens) # for masked LM\n",
    "        loss_lm = (loss_lm.float()).mean()\n",
    "        loss_clsf = criteria(logits_clsf, isNext) # for sentence classification\n",
    "        loss = loss_lm + loss_clsf\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# save model\n",
    "# torch.save(model.state_dict(), 'bert.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = batch[1]\n",
    "print(text)\n",
    "print([number_dict[w] for w in input_ids if number_dict[w] != '[PAD]'])\n",
    "\n",
    "logits_lm, logits_clsf = model(torch.LongTensor([input_ids]), torch.LongTensor([segment_ids]), torch.LongTensor([masked_pos]))\n",
    "logits_lm = logits_lm.data.max(2)[1][0].data.numpy()\n",
    "print('masked tokens list : ', [pos for pos in masked_tokens if pos != 0])\n",
    "print('predict masked tokens list : ', [pos for pos in logits_lm if pos != 0])\n",
    "\n",
    "logits_clsf = logits_clsf.data.max(1)[1].data.numpy()[0]\n",
    "print('isNext : ', True if isNext else False)\n",
    "print('predict isNext : ', True if logits_clsf else False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
