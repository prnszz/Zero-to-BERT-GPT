# Build BERT and GPT from 0ï¸âƒ£
[ğŸ‡¯ğŸ‡µæ—¥æœ¬èªç‰ˆ](./README_JP.md)


![](https://miro.medium.com/v2/resize:fit:1400/1*TzGwyi1TrqcIPV4WMU3sVg.png)

# [BERT](./Encoder/BERT/)
1. [Overview](./Encoder/BERT/overview.ipynb)
2. [Pre-training](./Encoder/BERT/pre-training.ipynb)
3. [Fine-tuning](./Encoder/BERT/fine-tuning.ipynb)
4. [Visualization-BertViz](https://github.com/jessevig/bertviz) 

# [Llama](./Decoder/Llama/)
1. [Overview](./Decoder/Llama/overview.ipynb)
2. [Pre-training](./Decoder/Llama/pre-training.ipynb)
3. [Fine-tuning](./Decoder/Llama/fine-tuning.ipynb)



# Quick Start
```
git clone \\
conda create -n "bert_gpt_test" python = 3.12
conda activate bert_gpt_test
pip install -r requirements.txt
```

### TO-DO
1. Roberta vs BERT
2. Encoder-Decoder: T5, BART
3. GPT, Llama